### Hi Folks! You happened to stop at this profile of.....

# Abhinav Reddy Pannala
#### Computer Science graduate exploring new technologies and solutions. Skilled in programming, algorithms, and problem-solving, with a collaborative mindset that values diverse perspectives. Proficient in Python Programming, and Data Science. Combines technical expertise and creativity to deliver innovative results, backed by a strong sense of responsibility and project integrity.


[ğŸ“– Master of Science in Computer Science](https://www.cise.ufl.edu/academics/graduate/masters-program/)
<br>
[ğŸ“ Bachelor of Technology in Computer Science & Engineering](https://jaipur.manipal.edu/fosta/department-of-Computer-Science-Engineering.php)
<br>
ğŸ“§ E-mail : [pannalaabhinav02@gmail.com](mailto:pannalaabhinav02@gmail.com)
<br>
ğŸ” LinkedIn  : [https://www.linkedin.com/in/abhinav-reddy18/](https://www.linkedin.com/in/abhinav-reddy18/)

## My Works are!!!

### ğŸ“Œ [Analyzers- Comparitive Study of legal Analysis using different AI Models](https://github.com/AbhinavReddy18-bytes/Analyzers-)
- **Data Collection**: Gathered penal code datasets for California and New York, performed web scraping, and created three versions (uncleaned, cleaned, combined) by manually fixing inconsistent rows and columns.
- **Preprocessing Steps**: Applied common techniques like normalization, tokenization, text cleaning, and vectorization to prepare the data for analysis.
BERT Model Testing: Experimented with a BERT model on both datasets, adding a â€œSTATEâ€ category for the combined dataset. Faced complications with memory usage and errors while training with torch.no_grad().
- **Training and Code Sharing**: Each model training iteration was set to 5 epochs, but iterative tuning would be costly. A code example is shared, focusing on GPT procedures rather than the full implementation.

### ğŸ“Œ [Reddit-Clone-and-Client-tester-and-simulator-with-REST-API](https://github.com/AbhinavReddy18-bytes/Reddit-Clone-and-Client-tester-and-simulator-with-REST-API-Project)
- **Engine**: Successfully implemented account registration, sub-reddit creation/join/leave, text posting, nested commenting, and up/downvoting with Karma tracking.
- **Tester/Simulator**: Created a simulator that models multiple users, handles connection/disconnection intervals, follows a Zipf distribution for sub-reddit memberships, and increases posting frequency for popular sub-reddits.
- **Architecture**: Deployed separate client processes for user actions and a single engine process for handling data. Collected performance metrics to assess system load and scalability.
- **REST API**: Developed a Reddit-like REST interface for the engine, along with a simple client that demonstrates all supported functionalities.

### ğŸ“Œ [Chord: P2P System and Simulation](https://github.com/AbhinavReddy18-bytes/Chord-P2P-System-and-Simulation-Distributed-Systems)
- **Node Creation**: Successfully created and managed individual nodes, each functioning as an actor within the network.  
- **Finger Table Setup**: Implemented efficient routing via finger tables, enabling quick lookups and streamlined message forwarding.  
- **Lookup Requests**: Each node performs lookups and routes messages correctly across the entire network, following the Chord protocol.  
- **Average Hops Calculation**: Computed and displayed the average number of hops needed for message delivery, confirming routing efficiency.  
- **Routing Mechanism**: Ensured accurate node responsibility and proper message forwarding in line with the Chord protocolâ€™s requirements.  
- **Largest Network Handled**: Validated the implementation with 3,000 nodes, each handling 3,000 requests, demonstrating scalability and robustness.  

### ğŸ“Œ [Actor Modelling](https://github.com/AbhinavReddy18-bytes/Distributed-Operating-Systems-Actor-Modelling)
- Distributed work units across multiple cores to boost parallel performance, experimenting with various unit sizes to balance communication overhead and execution speed.
- Measured CPU time against real time to gauge efficiency, and tuned performance by adjusting the number of workers and limiting sequence processing.
- Resolved actor model synchronization issues for smoother parallel execution, improving error handling and debugging with a focus on actor safety and reference capabilities.
- Ran extensive tests (n=1,000,000,000; k=20; workers=4) to validate system limits, noting that higher workload parameters increased execution times.

## I possess.....

- **_ğŸ’» Languages:_** Python, R, MySQL, GO, JavaScript, PhP, HTMl, CSS
- **_ğŸ’» Frameworks & Libraries:_** Django, React.js, TensorFlow, NumPy, Pandas, Keras, OpenCV
- **_ğŸ’» Data Analysis & Visualization:_** Tableau, Power BI, Matplotlib, Seaborn
- **_ğŸ’» Technologies:_** Machine Learning, Data Science, Big Data Analytics, Generative AI, Web Development, Hadoop, Spark
- **_ğŸ’» Development Tools:_** Git, Anaconda, Figma, Jupyter NoteBook, NetBeans, Visual Studio Code
- **_ğŸ’» Other Expertise:_** Agile Methodology, Data Structures, Cloud Computing (GCP)

## I worked at.....

- **ğŸ’¼ College of Veterinary Medicine, University of Florida** _, Data analyst_
